from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback
from transformers import create_optimizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
#from tensorflow.keras.utils import to_categorical
#import tensorflow as tf
import pandas as pd
import numpy as np
from pathlib import Path
from training import load_texts, load_data # Asumiendo que la definiste en training.py
from datasets import Dataset
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score
import torch
from torch.nn import CrossEntropyLoss
from sklearn.utils.class_weight import compute_class_weight

LABELED_DATA_PATH = Path('data/labeled/labeled_documents_36_750.csv')
# Par치metros del Modelo y Divisi칩n
TEST_SIZE = 0.15  # 15% para el conjunto de prueba final
VALIDATION_SIZE = 0.15 # 15% del total original para validaci칩n (aprox 0.1765 de lo que queda despu칠s del test split)
RANDOM_STATE = 42 # Para reproducibilidad
MAX_LENGTH=2048 #512

# --- Tokenizador y Dataset ---
#MODEL_NAME = "bert-base-multilingual-cased"
#MODEL_NAME = "PlanTL-GOB-ES/roberta-base-bne"
MODEL_NAME = "Narrativa/legal-longformer-base-4096-spanish"
MODEL_NAME = "mrm8488/longformer-base-4096-spanish"
#MODEL_NAME = "allenai/longformer-base-4096"

# --- Cargar datos ---
df = load_data(LABELED_DATA_PATH)
if df is None:
    raise SystemExit("No se pudieron cargar los datos")

# Codificar etiquetas
le = LabelEncoder()
df['label_id'] = le.fit_transform(df['label'])
label_mapping = dict(enumerate(le.classes_))

# Dividir conjunto
from sklearn.model_selection import train_test_split

X_paths = df['markdown_path']
y = df['label_id']

X_train_paths, X_test_paths, y_train_val, y_test = train_test_split(
    X_paths, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y
)
val_size_relative = VALIDATION_SIZE / (1.0 - TEST_SIZE)
X_train_paths, X_val_paths, y_train, y_val = train_test_split(
    X_train_paths, y_train_val, test_size=val_size_relative,
    random_state=RANDOM_STATE, stratify=y_train_val
)

X_train_texts = load_texts(X_train_paths)
X_val_texts = load_texts(X_val_paths)
X_test_texts = load_texts(X_test_paths)

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)

def smart_truncate_2(text, max_len=MAX_LENGTH):
    tokens = tokenizer.tokenize(text)
    if len(tokens) <= max_len:
        return text
    head = tokens[:max_len // 2]
    tail = tokens[-(max_len // 2):]
    truncated_tokens = head + tail
    return tokenizer.convert_tokens_to_string(truncated_tokens)

def smart_truncate(text, tokenizer, max_len=MAX_LENGTH, return_ids=False):
    encoding = tokenizer.encode_plus(
        text,
        add_special_tokens=False,
        return_attention_mask=False,
        return_token_type_ids=False
        # 丘멆잺 NO usar add_prefix_space aqu칤
    )

    input_ids = encoding["input_ids"]

    head = input_ids[:max_len // 2]
    tail = input_ids[-(max_len // 2):]
    truncated_ids = head + tail
    truncated_ids = input_ids
    if return_ids:
        return truncated_ids
    else:
        return tokenizer.decode(truncated_ids, skip_special_tokens=True)

'''
# Aplicarlo antes de tokenizar
X_train_texts = [smart_truncate_2(t) for t in X_train_texts]
X_val_texts = [smart_truncate_2(t) for t in X_val_texts]
X_test_texts = [smart_truncate_2(t) for t in X_test_texts]

'''
# Aplicarlo antes de tokenizar
X_train_texts = [smart_truncate(t, tokenizer) for t in X_train_texts]
X_val_texts = [smart_truncate(t, tokenizer) for t in X_val_texts]
X_test_texts = [smart_truncate(t, tokenizer) for t in X_test_texts]


train_dataset = Dataset.from_dict({"text": X_train_texts, "label": y_train})
val_dataset = Dataset.from_dict({"text": X_val_texts, "label": y_val})
test_dataset = Dataset.from_dict({"text": X_test_texts, "label": y_test})

# --- Tokenizar datasets ---
def tokenize_batch(example):
    return tokenizer(example["text"], padding="max_length", truncation=True, max_length=MAX_LENGTH)

train_dataset = train_dataset.map(tokenize_batch, batched=True)
val_dataset = val_dataset.map(tokenize_batch, batched=True)
test_dataset = test_dataset.map(tokenize_batch, batched=True)

# --- Convertir a tensores ---
columns = ['input_ids', 'attention_mask', 'label']
train_dataset.set_format(type='torch', columns=columns)
val_dataset.set_format(type='torch', columns=columns)
test_dataset.set_format(type='torch', columns=columns)

# --- Modelo ---
num_labels = len(le.classes_)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)

# --- Entrenamiento ---
training_args = TrainingArguments(
    output_dir="./models/bert_model//bert_classifier",
    eval_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    gradient_accumulation_steps=2,
    num_train_epochs=10,
    learning_rate=3e-5,
    weight_decay=0.01,
    warmup_ratio=0.1,  # Warmup para estabilizar
    logging_dir='./logs',
    logging_steps=10,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    fp16=True,
)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "precision": precision, "recall": recall, "f1": f1}

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],
)

class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)
class_weights = torch.tensor(class_weights, dtype=torch.float).to(model.device)

def custom_compute_loss(model, inputs, return_outputs=False, **kwargs):
    labels = inputs.pop("labels")
    outputs = model(**inputs)
    logits = outputs.logits
    loss_fct = CrossEntropyLoss()
    loss = loss_fct(logits, labels)
    return (loss, outputs) if return_outputs else loss

trainer.compute_loss = custom_compute_loss

trainer.train()

# --- Evaluaci칩n ---
preds = trainer.predict(test_dataset)
y_pred = preds.predictions.argmax(axis=-1)
print(classification_report(y_test, y_pred, target_names=le.classes_))



춰Claro que s칤! Implementar un an치lisis de errores dirigido es un paso fundamental para entender el comportamiento de tu modelo.
Para analizar si el modelo falla m치s en los documentos largos, debes realizar la evaluaci칩n sobre el conjunto de prueba, pero dividi칠ndolo en dos grupos:
 * Documentos "normales": El 90% de los documentos, cuya longitud es menor o igual al umbral que definiste.
 * Documentos "largos": El 10% de los documentos m치s extensos, que superan ese umbral.
Aqu칤 te muestro el c칩digo que puedes a침adir justo despu칠s de la l칤nea print(classification_report(...)) al final de tu script.
C칩digo para el An치lisis de Errores
Este bloque de c칩digo se encarga de realizar todo el an치lisis por ti.
print("\n" + "="*50)
print("INICIO DEL AN츼LISIS DE ERRORES POR LONGITUD")
print("="*50 + "\n")

# --- 1. Crear un DataFrame con todos los resultados del test set ---
# Es crucial usar los textos originales (X_test_texts) para medir la longitud ANTES de cualquier truncamiento.
df_results = pd.DataFrame({
    'text': X_test_texts,
    'true_label_id': y_test,
    'predicted_label_id': y_pred
})

# Mapear los IDs de las etiquetas a sus nombres para mayor claridad
df_results['true_label_name'] = df_results['true_label_id'].apply(lambda x: label_mapping[x])
df_results['predicted_label_name'] = df_results['predicted_label_id'].apply(lambda x: label_mapping[x])

# --- 2. Calcular la longitud en tokens de cada documento original ---
print("Calculando la longitud original en tokens de los documentos de prueba...")
df_results['token_length'] = [len(tokenizer.tokenize(t)) for t in df_results['text']]
print(f"Longitud media de tokens en el test set: {df_results['token_length'].mean():.2f}")

# --- 3. Definir el umbral y dividir el DataFrame ---
# Usamos el cuantil 0.9 para encontrar el umbral que separa el 10% m치s largo.
length_threshold = df_results['token_length'].quantile(0.90)
print(f"Umbral de longitud (cuantil 90): {length_threshold:.0f} tokens. Los documentos m치s largos que esto ser치n analizados por separado.")

# Dividir en dos grupos
df_long_docs = df_results[df_results['token_length'] > length_threshold]
df_normal_docs = df_results[df_results['token_length'] <= length_threshold]

print(f"\nTotal de documentos en Test: {len(df_results)}")
print(f"Documentos 'Normales' (<= {length_threshold:.0f} tokens): {len(df_normal_docs)}")
print(f"Documentos 'Largos' (> {length_threshold:.0f} tokens): {len(df_long_docs)}")

# --- 4. Generar y mostrar los reportes de clasificaci칩n para cada grupo ---

print("\n" + "-"*50)
print("## M칠tricas en DOCUMENTOS NORMALES (90% inferior) ##")
print("-"*50)
if not df_normal_docs.empty:
    report_normal = classification_report(
        df_normal_docs['true_label_id'],
        df_normal_docs['predicted_label_id'],
        target_names=le.classes_
    )
    print(report_normal)
else:
    print("No hay documentos en la categor칤a 'normal' para analizar.")

print("\n" + "-"*50)
print("## 游뚿 M칠tricas en DOCUMENTOS LARGOS (10% superior) 游뚿 ##")
print("-"*50)
if not df_long_docs.empty:
    report_long = classification_report(
        df_long_docs['true_label_id'],
        df_long_docs['predicted_label_id'],
        target_names=le.classes_
    )
    print(report_long)
else:
    print("No hay documentos en la categor칤a 'largo' para analizar.")

print("\n" + "="*50)
print("FIN DEL AN츼LISIS")
print("="*50)

# Opcional: Guardar los fallos en los documentos largos para una revisi칩n manual
errors_long_docs = df_long_docs[df_long_docs['true_label_id'] != df_long_docs['predicted_label_id']]
if not errors_long_docs.empty:
    errors_long_docs.to_csv("errores_en_documentos_largos.csv", index=False)
    print(f"\nSe han guardado {len(errors_long_docs)} errores de clasificaci칩n en documentos largos en 'errores_en_documentos_largos.csv' para revisi칩n manual.")


쮺칩mo funciona y c칩mo interpretar los resultados? 游븷
 * Medici칩n Correcta: El paso clave es medir la longitud en tokens del texto original, antes de que la funci칩n tokenize_batch lo trunque a MAX_LENGTH. Esto nos da la verdadera complejidad del documento.
 * Divisi칩n por Percentil: Usamos .quantile(0.90) para encontrar el valor de longitud que deja al 10% de los documentos m치s largos por encima y al 90% por debajo. Esto es m치s robusto que usar un n칰mero fijo.
 * Comparaci칩n Directa: El script imprime dos informes de clasificaci칩n uno al lado del otro.
Para interpretar los resultados, f칤jate en:
 * Ca칤da en F1-Score (weighted avg): Compara el f1-score promedio ponderado entre el informe de "Documentos Normales" y el de "Documentos Largos". Si ves una ca칤da significativa (ej. de 0.95 a 0.80), es una clara se침al de que el modelo sufre con los textos largos.
 * Precisi칩n y Recall por Clase: Observa si alguna clase espec칤fica empieza a tener un recall muy bajo en los documentos largos. Quiz치s el modelo confunde dos tipos de documentos largos porque la informaci칩n distintiva fue truncada.
 * Accuracy: Una ca칤da general en la accuracy tambi칠n es un indicador claro.
Si confirmas que el rendimiento es desproporcionadamente bajo en los documentos largos, tienes una justificaci칩n s칩lida para probar el modelo de 8192 tokens (clibrain/legal-longformer-base-8192-spanish), ya que la causa del error ser칤a la p칠rdida de informaci칩n por truncamiento.
Nota Adicional sobre tu C칩digo de Truncamiento
He notado una peque침a anomal칤a en tu funci칩n smart_truncate:
# En tu c칩digo original
def smart_truncate(text, tokenizer, max_len=MAX_LENGTH, return_ids=False):
    # ... (c칩digo para obtener head y tail) ...
    truncated_ids = head + tail
    truncated_ids = input_ids # <--- 춰Cuidado aqu칤!
    # ...

La l칤nea truncated_ids = input_ids est치 sobrescribiendo y anulando el truncamiento que calculaste en la l칤nea anterior. Esto significa que tu truncamiento "inteligente" no se est치 aplicando, y el truncamiento real lo hace tokenizer en el map (que por defecto corta por la derecha).
Para que funcione como esperas (mitad del principio y mitad del final), deber칤as eliminar esa l칤nea:
# Versi칩n corregida
def smart_truncate(text, tokenizer, max_len=MAX_LENGTH, return_ids=False):
    encoding = tokenizer.encode_plus(
        text,
        add_special_tokens=False,
    )
    input_ids = encoding["input_ids"]
    
    # Si no supera la longitud, no hagas nada
    if len(input_ids) <= max_len:
         return tokenizer.decode(input_ids, skip_special_tokens=True)

    head_len = max_len // 2
    tail_len = max_len - head_len

    head = input_ids[:head_len]
    tail = input_ids[-tail_len:]
    
    truncated_ids = head + tail # Ahora s칤 se usan los IDs truncados
    
    if return_ids:
        return truncated_ids
    else:
        return tokenizer.decode(truncated_ids, skip_special_tokens=True)

