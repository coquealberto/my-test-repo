from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback
from transformers import create_optimizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
#from tensorflow.keras.utils import to_categorical
#import tensorflow as tf
import pandas as pd
import numpy as np
from pathlib import Path
from training import load_texts, load_data # Asumiendo que la definiste en training.py
from datasets import Dataset
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score
import torch
from torch.nn import CrossEntropyLoss
from sklearn.utils.class_weight import compute_class_weight

LABELED_DATA_PATH = Path('data/labeled/labeled_documents_36_750.csv')
# Par√°metros del Modelo y Divisi√≥n
TEST_SIZE = 0.15  # 15% para el conjunto de prueba final
VALIDATION_SIZE = 0.15 # 15% del total original para validaci√≥n (aprox 0.1765 de lo que queda despu√©s del test split)
RANDOM_STATE = 42 # Para reproducibilidad
MAX_LENGTH=2048 #512

# --- Tokenizador y Dataset ---
#MODEL_NAME = "bert-base-multilingual-cased"
#MODEL_NAME = "PlanTL-GOB-ES/roberta-base-bne"
MODEL_NAME = "Narrativa/legal-longformer-base-4096-spanish"
MODEL_NAME = "mrm8488/longformer-base-4096-spanish"
#MODEL_NAME = "allenai/longformer-base-4096"

# --- Cargar datos ---
df = load_data(LABELED_DATA_PATH)
if df is None:
    raise SystemExit("No se pudieron cargar los datos")

# Codificar etiquetas
le = LabelEncoder()
df['label_id'] = le.fit_transform(df['label'])
label_mapping = dict(enumerate(le.classes_))

# Dividir conjunto
from sklearn.model_selection import train_test_split

X_paths = df['markdown_path']
y = df['label_id']

X_train_paths, X_test_paths, y_train_val, y_test = train_test_split(
    X_paths, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y
)
val_size_relative = VALIDATION_SIZE / (1.0 - TEST_SIZE)
X_train_paths, X_val_paths, y_train, y_val = train_test_split(
    X_train_paths, y_train_val, test_size=val_size_relative,
    random_state=RANDOM_STATE, stratify=y_train_val
)

X_train_texts = load_texts(X_train_paths)
X_val_texts = load_texts(X_val_paths)
X_test_texts = load_texts(X_test_paths)

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)

def smart_truncate_2(text, max_len=MAX_LENGTH):
    tokens = tokenizer.tokenize(text)
    if len(tokens) <= max_len:
        return text
    head = tokens[:max_len // 2]
    tail = tokens[-(max_len // 2):]
    truncated_tokens = head + tail
    return tokenizer.convert_tokens_to_string(truncated_tokens)

def smart_truncate(text, tokenizer, max_len=MAX_LENGTH, return_ids=False):
    encoding = tokenizer.encode_plus(
        text,
        add_special_tokens=False,
        return_attention_mask=False,
        return_token_type_ids=False
        # ‚ö†Ô∏è NO usar add_prefix_space aqu√≠
    )

    input_ids = encoding["input_ids"]

    head = input_ids[:max_len // 2]
    tail = input_ids[-(max_len // 2):]
    truncated_ids = head + tail
    truncated_ids = input_ids
    if return_ids:
        return truncated_ids
    else:
        return tokenizer.decode(truncated_ids, skip_special_tokens=True)

'''
# Aplicarlo antes de tokenizar
X_train_texts = [smart_truncate_2(t) for t in X_train_texts]
X_val_texts = [smart_truncate_2(t) for t in X_val_texts]
X_test_texts = [smart_truncate_2(t) for t in X_test_texts]

'''
# Aplicarlo antes de tokenizar
X_train_texts = [smart_truncate(t, tokenizer) for t in X_train_texts]
X_val_texts = [smart_truncate(t, tokenizer) for t in X_val_texts]
X_test_texts = [smart_truncate(t, tokenizer) for t in X_test_texts]


train_dataset = Dataset.from_dict({"text": X_train_texts, "label": y_train})
val_dataset = Dataset.from_dict({"text": X_val_texts, "label": y_val})
test_dataset = Dataset.from_dict({"text": X_test_texts, "label": y_test})

# --- Tokenizar datasets ---
def tokenize_batch(example):
    return tokenizer(example["text"], padding="max_length", truncation=True, max_length=MAX_LENGTH)

train_dataset = train_dataset.map(tokenize_batch, batched=True)
val_dataset = val_dataset.map(tokenize_batch, batched=True)
test_dataset = test_dataset.map(tokenize_batch, batched=True)

# --- Convertir a tensores ---
columns = ['input_ids', 'attention_mask', 'label']
train_dataset.set_format(type='torch', columns=columns)
val_dataset.set_format(type='torch', columns=columns)
test_dataset.set_format(type='torch', columns=columns)

# --- Modelo ---
num_labels = len(le.classes_)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)

# --- Entrenamiento ---
training_args = TrainingArguments(
    output_dir="./models/bert_model//bert_classifier",
    eval_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    gradient_accumulation_steps=2,
    num_train_epochs=10,
    learning_rate=3e-5,
    weight_decay=0.01,
    warmup_ratio=0.1,  # Warmup para estabilizar
    logging_dir='./logs',
    logging_steps=10,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    fp16=True,
)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "precision": precision, "recall": recall, "f1": f1}

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],
)

class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)
class_weights = torch.tensor(class_weights, dtype=torch.float).to(model.device)

def custom_compute_loss(model, inputs, return_outputs=False, **kwargs):
    labels = inputs.pop("labels")
    outputs = model(**inputs)
    logits = outputs.logits
    loss_fct = CrossEntropyLoss()
    loss = loss_fct(logits, labels)
    return (loss, outputs) if return_outputs else loss

trainer.compute_loss = custom_compute_loss

trainer.train()

# --- Evaluaci√≥n ---
preds = trainer.predict(test_dataset)
y_pred = preds.predictions.argmax(axis=-1)
print(classification_report(y_test, y_pred, target_names=le.classes_))



¬°Claro que s√≠! Implementar un an√°lisis de errores dirigido es un paso fundamental para entender el comportamiento de tu modelo.
Para analizar si el modelo falla m√°s en los documentos largos, debes realizar la evaluaci√≥n sobre el conjunto de prueba, pero dividi√©ndolo en dos grupos:
 * Documentos "normales": El 90% de los documentos, cuya longitud es menor o igual al umbral que definiste.
 * Documentos "largos": El 10% de los documentos m√°s extensos, que superan ese umbral.
Aqu√≠ te muestro el c√≥digo que puedes a√±adir justo despu√©s de la l√≠nea print(classification_report(...)) al final de tu script.
C√≥digo para el An√°lisis de Errores
Este bloque de c√≥digo se encarga de realizar todo el an√°lisis por ti.
print("\n" + "="*50)
print("INICIO DEL AN√ÅLISIS DE ERRORES POR LONGITUD")
print("="*50 + "\n")

# --- 1. Crear un DataFrame con todos los resultados del test set ---
# Es crucial usar los textos originales (X_test_texts) para medir la longitud ANTES de cualquier truncamiento.
df_results = pd.DataFrame({
    'text': X_test_texts,
    'true_label_id': y_test,
    'predicted_label_id': y_pred
})

# Mapear los IDs de las etiquetas a sus nombres para mayor claridad
df_results['true_label_name'] = df_results['true_label_id'].apply(lambda x: label_mapping[x])
df_results['predicted_label_name'] = df_results['predicted_label_id'].apply(lambda x: label_mapping[x])

# --- 2. Calcular la longitud en tokens de cada documento original ---
print("Calculando la longitud original en tokens de los documentos de prueba...")
df_results['token_length'] = [len(tokenizer.tokenize(t)) for t in df_results['text']]
print(f"Longitud media de tokens en el test set: {df_results['token_length'].mean():.2f}")

# --- 3. Definir el umbral y dividir el DataFrame ---
# Usamos el cuantil 0.9 para encontrar el umbral que separa el 10% m√°s largo.
length_threshold = df_results['token_length'].quantile(0.90)
print(f"Umbral de longitud (cuantil 90): {length_threshold:.0f} tokens. Los documentos m√°s largos que esto ser√°n analizados por separado.")

# Dividir en dos grupos
df_long_docs = df_results[df_results['token_length'] > length_threshold]
df_normal_docs = df_results[df_results['token_length'] <= length_threshold]

print(f"\nTotal de documentos en Test: {len(df_results)}")
print(f"Documentos 'Normales' (<= {length_threshold:.0f} tokens): {len(df_normal_docs)}")
print(f"Documentos 'Largos' (> {length_threshold:.0f} tokens): {len(df_long_docs)}")

# --- 4. Generar y mostrar los reportes de clasificaci√≥n para cada grupo ---

print("\n" + "-"*50)
print("## M√©tricas en DOCUMENTOS NORMALES (90% inferior) ##")
print("-"*50)
if not df_normal_docs.empty:
    report_normal = classification_report(
        df_normal_docs['true_label_id'],
        df_normal_docs['predicted_label_id'],
        target_names=le.classes_
    )
    print(report_normal)
else:
    print("No hay documentos en la categor√≠a 'normal' para analizar.")

print("\n" + "-"*50)
print("## üö® M√©tricas en DOCUMENTOS LARGOS (10% superior) üö® ##")
print("-"*50)
if not df_long_docs.empty:
    report_long = classification_report(
        df_long_docs['true_label_id'],
        df_long_docs['predicted_label_id'],
        target_names=le.classes_
    )
    print(report_long)
else:
    print("No hay documentos en la categor√≠a 'largo' para analizar.")

print("\n" + "="*50)
print("FIN DEL AN√ÅLISIS")
print("="*50)

# Opcional: Guardar los fallos en los documentos largos para una revisi√≥n manual
errors_long_docs = df_long_docs[df_long_docs['true_label_id'] != df_long_docs['predicted_label_id']]
if not errors_long_docs.empty:
    errors_long_docs.to_csv("errores_en_documentos_largos.csv", index=False)
    print(f"\nSe han guardado {len(errors_long_docs)} errores de clasificaci√≥n en documentos largos en 'errores_en_documentos_largos.csv' para revisi√≥n manual.")


¬øC√≥mo funciona y c√≥mo interpretar los resultados? üßê
 * Medici√≥n Correcta: El paso clave es medir la longitud en tokens del texto original, antes de que la funci√≥n tokenize_batch lo trunque a MAX_LENGTH. Esto nos da la verdadera complejidad del documento.
 * Divisi√≥n por Percentil: Usamos .quantile(0.90) para encontrar el valor de longitud que deja al 10% de los documentos m√°s largos por encima y al 90% por debajo. Esto es m√°s robusto que usar un n√∫mero fijo.
 * Comparaci√≥n Directa: El script imprime dos informes de clasificaci√≥n uno al lado del otro.
Para interpretar los resultados, f√≠jate en:
 * Ca√≠da en F1-Score (weighted avg): Compara el f1-score promedio ponderado entre el informe de "Documentos Normales" y el de "Documentos Largos". Si ves una ca√≠da significativa (ej. de 0.95 a 0.80), es una clara se√±al de que el modelo sufre con los textos largos.
 * Precisi√≥n y Recall por Clase: Observa si alguna clase espec√≠fica empieza a tener un recall muy bajo en los documentos largos. Quiz√°s el modelo confunde dos tipos de documentos largos porque la informaci√≥n distintiva fue truncada.
 * Accuracy: Una ca√≠da general en la accuracy tambi√©n es un indicador claro.
Si confirmas que el rendimiento es desproporcionadamente bajo en los documentos largos, tienes una justificaci√≥n s√≥lida para probar el modelo de 8192 tokens (clibrain/legal-longformer-base-8192-spanish), ya que la causa del error ser√≠a la p√©rdida de informaci√≥n por truncamiento.
Nota Adicional sobre tu C√≥digo de Truncamiento
He notado una peque√±a anomal√≠a en tu funci√≥n smart_truncate:
# En tu c√≥digo original
def smart_truncate(text, tokenizer, max_len=MAX_LENGTH, return_ids=False):
    # ... (c√≥digo para obtener head y tail) ...
    truncated_ids = head + tail
    truncated_ids = input_ids # <--- ¬°Cuidado aqu√≠!
    # ...

La l√≠nea truncated_ids = input_ids est√° sobrescribiendo y anulando el truncamiento que calculaste en la l√≠nea anterior. Esto significa que tu truncamiento "inteligente" no se est√° aplicando, y el truncamiento real lo hace tokenizer en el map (que por defecto corta por la derecha).
Para que funcione como esperas (mitad del principio y mitad del final), deber√≠as eliminar esa l√≠nea:
# Versi√≥n corregida
def smart_truncate(text, tokenizer, max_len=MAX_LENGTH, return_ids=False):
    encoding = tokenizer.encode_plus(
        text,
        add_special_tokens=False,
    )
    input_ids = encoding["input_ids"]
    
    # Si no supera la longitud, no hagas nada
    if len(input_ids) <= max_len:
         return tokenizer.decode(input_ids, skip_special_tokens=True)

    head_len = max_len // 2
    tail_len = max_len - head_len

    head = input_ids[:head_len]
    tail = input_ids[-tail_len:]
    
    truncated_ids = head + tail # Ahora s√≠ se usan los IDs truncados
    
    if return_ids:
        return truncated_ids
    else:
        return tokenizer.decode(truncated_ids, skip_special_tokens=True)

