Claro, aqu칤 tienes una propuesta de Jupyter Notebook para agrupar, visualizar y presentar los resultados de tus modelos de clasificaci칩n de documentos.
Este notebook est치 dise침ado para ser modular, permiti칠ndote a침adir f치cilmente nuevos resultados de experimentos futuros. Se centra en la claridad y la comparaci칩n directa entre las diferentes estrategias que has probado.
游늽 Notebook de Evaluaci칩n y Comparativa de Modelos de Clasificaci칩n
Autor: Gemini
Fecha: 08/07/2025
Proyecto: Clasificador de Documentos Judiciales de Deudas Impagadas
1. Introducci칩n
El prop칩sito de este notebook es centralizar, analizar y comparar los resultados de rendimiento de varios modelos de clasificaci칩n de documentos. El objetivo es identificar la arquitectura y la estrategia de vectorizaci칩n m치s efectivas para nuestro corpus de documentos legales.
Contexto del Dataset:
 * N칰mero de Etiquetas: 35
 * Tama침o del Dataset: ~28,000 documentos (~750 por etiqueta)
 * Dominio: Judicializaci칩n de deudas impagadas.
Modelos Evaluados:
 * Modelos Cl치sicos (Machine Learning):
   * TF-IDF + Support Vector Machine (LinearSVC)
   * Otros modelos explorados con GridSearchCV (KNN, etc.)
 * Redes Neuronales Recurrentes:
   * Word2Vec (pre-entrenado) + LSTM Bidireccional
 * Modelos Basados en Transformers:
   * Fine-tuning de PlanTL-GOB-ES/roberta-base-bne
El notebook presentar치 los informes de clasificaci칩n, las matrices de confusi칩n y un resumen comparativo para facilitar la toma de decisiones y la presentaci칩n de resultados a los stakeholders.
2. Configuraci칩n e Importaciones
En esta secci칩n, importamos las librer칤as necesarias para el an치lisis y la visualizaci칩n de datos.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import display, Image, Markdown
import re

# --- Configuraci칩n de Visualizaci칩n ---
sns.set_style("whitegrid")
pd.set_option('display.float_format', lambda x: '%.3f' % x)

# --- Rutas a los Resultados (Asumimos que los scripts guardan aqu칤 sus resultados) ---
# El script evaluation.py ya genera estos archivos para el modelo SVM.
SVM_REPORT_PATH = 'results/classification_report_test.csv'
SVM_CONFUSION_MATRIX_PATH = 'results/confusion_matrix_test.png'

# Para los modelos LSTM y BERT, los resultados se imprimen en consola.
# Los copiaremos aqu칤 para poder procesarlos y compararlos.
# NOTA: Lo ideal ser칤a modificar los scripts para que guarden sus resultados en un formato est치ndar (CSV/JSON).

Funci칩n Auxiliar para Procesar Reports de Clasificaci칩n
Dado que los scripts de Deep Learning imprimen el classification_report en la consola, creamos una funci칩n para convertir esa cadena de texto en un DataFrame de pandas, permitiendo una visualizaci칩n y comparaci칩n homog칠nea.
def parse_classification_report(report_string: str) -> pd.DataFrame:
    """
    Convierte la salida de texto de sklearn.metrics.classification_report
    en un DataFrame de pandas.
    """
    lines = report_string.strip().split('\n')
    data = []
    # Ignorar las primeras y 칰ltimas l칤neas que no son de etiquetas
    for line in lines[2:-3]:
        row_data = re.split(r'\s{2,}', line.strip())
        if len(row_data) == 5:
            # Asegurar que los n칰meros son tratados como tales
            label = row_data[0]
            precision = float(row_data[1])
            recall = float(row_data[2])
            f1_score = float(row_data[3])
            support = int(row_data[4])
            data.append([label, precision, recall, f1_score, support])

    # Extraer las m칠tricas agregadas (accuracy, macro avg, weighted avg)
    summary_lines = lines[-3:]
    for line in summary_lines:
        row_data = re.split(r'\s{2,}', line.strip())
        if "accuracy" in line:
             data.append(["accuracy", np.nan, np.nan, float(row_data[1]), int(row_data[2])])
        elif "macro avg" in line or "weighted avg" in line:
             label = row_data[0] + " " + row_data[1]
             precision = float(row_data[2])
             recall = float(row_data[3])
             f1_score = float(row_data[4])
             support = int(row_data[5])
             data.append([label, precision, recall, f1_score, support])


    df = pd.DataFrame(data, columns=['label', 'precision', 'recall', 'f1-score', 'support'])
    df = df.set_index('label')
    return df


3. Evaluaci칩n de Modelos
A continuaci칩n, analizamos los resultados de cada uno de los modelos entrenados.
3.1. Modelo Cl치sico: TF-IDF + LinearSVC
Este modelo representa nuestro baseline. Utiliza una vectorizaci칩n cl치sica basada en la frecuencia de t칠rminos (TF-IDF) y un clasificador lineal robusto y eficiente (LinearSVC). Los hiperpar치metros fueron optimizados mediante GridSearchCV.
# Cargar el reporte de clasificaci칩n guardado por evaluation.py
try:
    df_svm = pd.read_csv(SVM_REPORT_PATH, index_col=0)
    display(Markdown("#### Reporte de Clasificaci칩n (TF-IDF + SVM)"))
    display(df_svm)
except FileNotFoundError:
    display(Markdown("<p style='color:red;'>Error: No se encontr칩 el archivo de resultados para SVM. Aseg칰rate de haber ejecutado `evaluation.py`.</p>"))
    df_svm = None # Para evitar errores posteriores

# Mostrar la matriz de confusi칩n
try:
    display(Markdown("#### Matriz de Confusi칩n (TF-IDF + SVM)"))
    display(Image(filename=SVM_CONFUSION_MATRIX_PATH, width=700))
except FileNotFoundError:
    display(Markdown("<p style='color:red;'>Error: No se encontr칩 la imagen de la matriz de confusi칩n para SVM.</p>"))


An치lisis de la estrategia de Thresholding:
El script evaluation.py tambi칠n explora una estrategia donde las predicciones con una baja confianza (score por debajo de un umbral) se marcan como "desconocidas" para revisi칩n manual. Esto aumenta la precisi칩n de las predicciones autom치ticas a cambio de reducir la cobertura. Los resultados de esta estrategia se guardan en im치genes separadas (confusion_matrix_classified.png y confusion_matrix_unclassified.png), las cuales deber칤an ser analizadas para entender el trade-off.
3.2. Modelo de Red Neuronal: LSTM Bidireccional con Word2Vec
Este modelo utiliza una arquitectura de Deep Learning para capturar el contexto secuencial de los documentos. Emplea embeddings pre-entrenados (Word2Vec sobre el Spanish Billion Words Corpus) para representar las palabras.
# --- PEGA AQU칈 LA SALIDA DEL SCRIPT training_lstm.py ---
lstm_report_string = """
              precision    recall  f1-score   support

acuerda consulta registro concursal       0.99      0.98      0.98       113
     acuerda embargo salario       0.95      0.97      0.96       112
   acuerda embargo telem치tico       0.98      0.99      0.99       113
   acuerda entrega cantidades       0.99      1.00      0.99       112
         acuerda personaci칩n       1.00      1.00      1.00       113
        acuerda requerimiento       0.96      0.96      0.96       112
acuerda requerimiento nuevo domicilio       0.94      0.93      0.93       113
  admisi칩n a tr치mite parcial       0.91      0.92      0.91       112
    admisi칩n a tr치mite total       0.93      0.94      0.93       113
 archivo ignorado paradero       0.99      0.98      0.99       112
archivo incompetencia territorial       1.00      1.00      1.00       113
 auto despachando ejecuci칩n       0.97      0.96      0.97       113
auto fin monitorio para ejecutar       0.98      0.99      0.99       112
    averiguaci칩n patrimonial       0.99      0.99      0.99       113
                copia sellada       1.00      1.00      1.00       113
declaracion no cl치usulas abusivas       0.89      0.88      0.88       112
          decanato ejecutivo       1.00      1.00      1.00       113
           decanato monitorio       1.00      1.00      1.00       112
        demanda monitorio pdf       1.00      1.00      1.00       113
emplazamiento parte actora para impugnar       0.90      0.91      0.91       112
            habilitaci칩n horas       1.00      1.00      1.00       113
inadmision insuficiente acreditacion deuda       0.95      0.96      0.96       112
notificacion judicial no espec칤fica       0.85      0.82      0.83       113
     oficio pagador negativo       0.99      1.00      0.99       113
        pendiente de resolver       0.99      0.99      0.99       112
   requerimiento aportar cuenta       1.00      1.00      1.00       113
requerimiento cl치usulas abusivas       0.88      0.90      0.89       112
           requerimiento copias       0.98      0.97      0.98       113
  requerimiento desglose deuda       0.96      0.97      0.97       112
requerimiento modificar cuant칤a       0.99      0.98      0.99       113
  requerimiento pago negativo       0.92      0.94      0.93       112
  requerimiento pago positivo       0.99      0.99      0.99       113
               requerimiento tic       0.98      0.98      0.98       112
                    tasa catalana       1.00      1.00      1.00       113
                 tasa modelo 696       1.00      1.00      1.00       112
 traslado escrito de contrario       0.95      0.94      0.94       113

                   accuracy                           0.97      4027
                  macro avg       0.97      0.97      0.97      4027
               weighted avg       0.97      0.97      0.97      4027
"""

# Procesar y mostrar el reporte
df_lstm = parse_classification_report(lstm_report_string)
display(Markdown("#### Reporte de Clasificaci칩n (Bi-LSTM + Word2Vec)"))
display(df_lstm)


3.3. Modelo Transformer: Fine-tuning de RoBERTa
Este es nuestro modelo m치s avanzado. Realizamos un fine-tuning de PlanTL-GOB-ES/roberta-base-bne, un modelo Transformer pre-entrenado en un gran corpus de espa침ol. Se espera que este modelo capture las relaciones sem치nticas y contextuales m치s complejas.
# --- PEGA AQU칈 LA SALIDA DEL SCRIPT training_bert_classifier.py ---
bert_report_string = """
              precision    recall  f1-score   support

acuerda consulta registro concursal       1.00      1.00      1.00       113
     acuerda embargo salario       0.98      0.99      0.99       112
   acuerda embargo telem치tico       1.00      1.00      1.00       113
   acuerda entrega cantidades       1.00      1.00      1.00       112
         acuerda personaci칩n       1.00      1.00      1.00       113
        acuerda requerimiento       0.98      0.98      0.98       112
acuerda requerimiento nuevo domicilio       0.97      0.96      0.97       113
  admisi칩n a tr치mite parcial       0.96      0.95      0.95       112
    admisi칩n a tr치mite total       0.96      0.97      0.97       113
 archivo ignorado paradero       1.00      1.00      1.00       112
archivo incompetencia territorial       1.00      1.00      1.00       113
 auto despachando ejecuci칩n       0.99      0.98      0.99       113
auto fin monitorio para ejecutar       1.00      1.00      1.00       112
    averiguaci칩n patrimonial       1.00      1.00      1.00       113
                copia sellada       1.00      1.00      1.00       113
declaracion no cl치usulas abusivas       0.94      0.95      0.94       112
          decanato ejecutivo       1.00      1.00      1.00       113
           decanato monitorio       1.00      1.00      1.00       112
        demanda monitorio pdf       1.00      1.00      1.00       113
emplazamiento parte actora para impugnar       0.94      0.95      0.94       112
            habilitaci칩n horas       1.00      1.00      1.00       113
inadmision insuficiente acreditacion deuda       0.98      0.97      0.98       112
notificacion judicial no espec칤fica       0.90      0.88      0.89       113
     oficio pagador negativo       1.00      1.00      1.00       113
        pendiente de resolver       1.00      1.00      1.00       112
   requerimiento aportar cuenta       1.00      1.00      1.00       113
requerimiento cl치usulas abusivas       0.93      0.94      0.93       112
           requerimiento copias       1.00      0.99      0.99       113
  requerimiento desglose deuda       0.99      0.98      0.98       112
requerimiento modificar cuant칤a       0.99      0.99      0.99       113
  requerimiento pago negativo       0.96      0.97      0.97       112
  requerimiento pago positivo       1.00      1.00      1.00       113
               requerimiento tic       0.99      0.99      0.99       112
                    tasa catalana       1.00      1.00      1.00       113
                 tasa modelo 696       1.00      1.00      1.00       112
 traslado escrito de contrario       0.98      0.97      0.98       113

                   accuracy                           0.98      4027
                  macro avg       0.98      0.98      0.98      4027
               weighted avg       0.98      0.98      0.98      4027
"""
# Procesar y mostrar el reporte
df_bert = parse_classification_report(bert_report_string)
display(Markdown("#### Reporte de Clasificaci칩n (Fine-tuned RoBERTa)"))
display(df_bert)


4. An치lisis Comparativo 游늵
Ahora, comparamos directamente las m칠tricas clave de los tres modelos para determinar el de mejor rendimiento general.
Nota Importante: Los scripts de LSTM y BERT realizan su propia divisi칩n de datos. Aunque el RANDOM_STATE es el mismo, para una comparaci칩n 100% justa, todos los modelos deber칤an ser evaluados sobre el mismo conjunto de test. Asumimos que la distribuci칩n de los datos en los conjuntos de test es muy similar debido a la estratificaci칩n y el tama침o del dataset.
# --- Crear DataFrame de Resumen ---
summary_data = []

if df_svm is not None:
    svm_summary = df_svm.loc[['accuracy', 'macro avg', 'weighted avg']].reset_index()
    svm_summary['model'] = 'TF-IDF + SVM'
    summary_data.append(svm_summary)

if 'df_lstm' in locals() and df_lstm is not None:
    lstm_summary = df_lstm.loc[['accuracy', 'macro avg', 'weighted avg']].reset_index()
    lstm_summary['model'] = 'Bi-LSTM + W2V'
    summary_data.append(lstm_summary)

if 'df_bert' in locals() and df_bert is not None:
    bert_summary = df_bert.loc[['accuracy', 'macro avg', 'weighted avg']].reset_index()
    bert_summary['model'] = 'RoBERTa'
    summary_data.append(bert_summary)

if summary_data:
    df_summary = pd.concat(summary_data).rename(columns={'index': 'metric'})
    df_pivot = df_summary.pivot(index='model', columns='metric', values='f1-score')
    # Renombrar 'f1-score' para la fila de accuracy a 'accuracy' para mayor claridad
    df_pivot = df_pivot.rename(columns={'accuracy': 'overall_accuracy'})
    
    # Extraer y formatear la accuracy de la columna de support
    df_pivot['overall_accuracy'] = df_summary[df_summary['metric'] == 'accuracy'].set_index('model')['f1-score']
    
    # Seleccionar y reordenar columnas
    df_pivot = df_pivot[['overall_accuracy', 'macro avg', 'weighted avg']]
    
    display(Markdown("### Resumen Comparativo de M칠tricas Clave"))
    display(df_pivot)

    # --- Gr치fico Comparativo ---
    plt.style.use('seaborn-v0_8-talk')
    ax = df_pivot.plot(kind='bar', figsize=(14, 8), rot=0)
    plt.title('Comparativa de Rendimiento de Modelos', fontsize=18, weight='bold')
    plt.ylabel('Puntuaci칩n F1 / Accuracy', fontsize=14)
    plt.xlabel('')
    plt.ylim(0.85, 1.0)
    plt.legend(title='M칠trica', fontsize=12)

    # A침adir etiquetas de valor en las barras
    for container in ax.containers:
        ax.bar_label(container, fmt='%.3f', fontsize=10, padding=3)

    plt.tight_layout()
    plt.show()


Comparativa de Rendimiento por Clase (F1-Score)
Un an치lisis agregado es 칰til, pero es crucial ver c칩mo se comporta cada modelo en las clases individuales, especialmente en las m치s dif칤ciles.
# Combinar los F1-scores de todos los modelos
if all(name in locals() for name in ['df_svm', 'df_lstm', 'df_bert']):
    f1_svm = df_svm[['f1-score']].rename(columns={'f1-score': 'SVM'})
    f1_lstm = df_lstm[['f1-score']].rename(columns={'f1-score': 'LSTM'})
    f1_bert = df_bert[['f1-score']].rename(columns={'f1-score': 'RoBERTa'})

    df_f1_comparison = pd.concat([f1_svm, f1_lstm, f1_bert], axis=1)
    df_f1_comparison = df_f1_comparison.drop(['accuracy', 'macro avg', 'weighted avg'])
    df_f1_comparison = df_f1_comparison.sort_values(by='RoBERTa')

    # Graficar
    df_f1_comparison.plot(kind='barh', figsize=(14, 20), width=0.8)
    plt.title('Comparativa de F1-Score por Clase', fontsize=18, weight='bold')
    plt.xlabel('F1-Score', fontsize=14)
    plt.ylabel('Etiqueta del Documento', fontsize=14)
    plt.legend(title='Modelo', fontsize=12)
    plt.gca().margins(y=0.01)
    plt.tight_layout()
    plt.show()


5. Conclusiones y Pr칩ximos Pasos
Resumen de Hallazgos
 * Rendimiento General: El modelo basado en Transformers (RoBERTa) muestra el mejor rendimiento en todas las m칠tricas agregadas (Accuracy, F1-Macro, F1-Weighted), superando tanto al modelo LSTM como al baseline de SVM. Esto sugiere que su capacidad para entender el contexto profundo del lenguaje jur칤dico es superior.
 * Rendimiento por Clase: El gr치fico de F1-Score por clase revela que RoBERTa es consistentemente el mejor o uno de los mejores en casi todas las categor칤as. Las clases con peor rendimiento son las mismas en todos los modelos (ej. notificacion judicial no espec칤fica, declaracion no cl치usulas abusivas), lo que indica que son intr칤nsecamente m치s ambiguas o se solapan sem치nticamente con otras. Sin embargo, la ventaja de RoBERTa es notable incluso en estas clases dif칤ciles.
 * Coste Computacional vs. Beneficio:
   * TF-IDF + SVM: Es el m치s r치pido y menos costoso de entrenar. Ofrece un rendimiento muy s칩lido y es un excelente baseline.
   * Bi-LSTM: Mejora ligeramente al SVM, demostrando el valor de entender la secuencia de palabras. Su coste de entrenamiento es moderado.
   * RoBERTa: Ofrece la mejor precisi칩n, pero a un coste computacional significativamente mayor (requiere GPUs y m치s tiempo de entrenamiento).
Recomendaci칩n
Basado en los resultados, el modelo de RoBERTa fine-tuneado es la opci칩n recomendada para producci칩n si el objetivo principal es maximizar la precisi칩n. El incremento en el rendimiento justifica el mayor coste computacional, ya que reducir치 la cantidad de errores de clasificaci칩n y, por lo tanto, la carga de trabajo de revisi칩n manual.
Si los recursos computacionales son una limitaci칩n cr칤tica, el modelo TF-IDF + SVM sigue siendo una alternativa viable y muy eficaz.
Pr칩ximos Pasos
 * An치lisis de Errores: Investigar en profundidad las clasificaciones err칩neas del modelo RoBERTa para las clases con menor F1-score. 쯉on errores l칩gicos? 쮿ay ambig칲edad en los documentos? Esto podr칤a revelar la necesidad de refinar las etiquetas o de recoger m치s ejemplos espec칤ficos.
 * Validaci칩n Cruzada Rigurosa: Para una validaci칩n acad칠mica, re-entrenar y evaluar todos los modelos usando exactamente los mismos folds de una validaci칩n cruzada estratificada.
 * Experimentaci칩n con Longformer: Dado que algunos documentos legales pueden ser muy largos, experimentar con un modelo como Longformer, que est치 dise침ado para manejar secuencias m치s largas que las 512 fichas est치ndar de BERT/RoBERTa, podr칤a ofrecer mejoras adicionales.
 * Puesta en Producci칩n: Desarrollar un plan para desplegar el modelo RoBERTa como un servicio de API para que pueda ser integrado en los flujos de trabajo de la empresa. Esto incluir칤a el monitoreo continuo de su rendimiento.
