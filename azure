¡Excelente! Integrar un servicio de OCR como Azure Document Intelligence es el paso lógico y profesional para manejar PDFs escaneados o con texto corrupto. La manera que propones de agrupar las páginas anómalas y procesarlas en un solo lote es muy eficiente para minimizar las llamadas a la API.
A continuación te muestro la mejor manera de integrar este flujo en tu script src/preprocessing.py, manteniendo la lógica existente y añadiendo esta nueva capacidad de forma robusta y condicional.
1. Actualiza requirements.txt
Añade las bibliotecas de Azure:
pandas>=1.3.0
scikit-learn>=1.0.0
joblib>=1.0.0
matplotlib>=3.4.0
seaborn>=0.11.0
markitdown
pypdf>=4.0.0
azure-ai-documentintelligence # Añadido
azure-core # Añadido

Y ejecuta pip install -r requirements.txt para instalarlas.
2. Modificaciones en src/preprocessing.py
Vamos a reestructurar el script para que incorpore tus funciones y la nueva lógica.
# --- Nuevas importaciones ---
import os
import subprocess
import pandas as pd
from pathlib import Path
import logging
import re
from pypdf import PdfReader, PdfWriter
import shutil
import tempfile # Necesario para las funciones de Azure
from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import DocumentIntelligenceClient
# from azure.ai.documentintelligence.models import DocumentAnalysisFeature # No se usa en tu código, la comento

# --- Configuración del logging (sin cambios) ---
# ...

# --- Constantes (sin cambios) ---
# ...

# >>> INICIO: Pega aquí tus 4 funciones de Azure <<<
def create_azure_client(endpoint, key):
    """ Crea y retorna un cliente de Azure Document Intelligence. """
    logging.info("Creando cliente de Azure Document Intelligence...")
    return DocumentIntelligenceClient(endpoint=endpoint, credential=AzureKeyCredential(key))

def analyze_document_read(client, file_path):
    """ Analiza un documento con el modelo read. """
    logging.info(f"Enviando '{Path(file_path).name}' a Azure Document Intelligence (modelo 'read')...")
    with open(file_path, "rb") as pdf:
        poller = client.begin_analyze_document(
            model_id="prebuilt-read",
            body=pdf,
            content_type="application/pdf",
            locale="es-ES" # Especificar español es una buena práctica
        )
    result = poller.result()
    logging.info("Análisis de Azure completado.")
    return result

def is_anomalous_text(text):
    """Detecta si el texto extraído es anómalo."""
    # Tu lógica aquí es un buen punto de partida.
    if not text or not text.strip():
        return True
    
    # Si la página es muy corta, no es anómala si contiene el aviso legal típico.
    if len(text.strip()) < 300 and "Este documento es una copia auténtica del documento" not in text:
        return True
    
    # Comprobar proporción de caracteres no alfanuméricos
    # (puede dar falsos positivos en texto con muchos símbolos, como '€', '/', etc.)
    try:
        non_alpha_chars = sum(1 for c in text if not c.isalnum() and not c.isspace())
        total_chars = len(text)
        if total_chars > 0 and (non_alpha_chars / total_chars > 0.30): # Ajustado a 30% para ser más permisivo
            logging.info("Texto marcado como anómalo por alta proporción de caracteres no alfanuméricos.")
            return True
    except ZeroDivisionError:
        return True
        
    return False

def process_pages_with_ocr(anomalous_pages_indices, reader, client):
    """Procesa un grupo de páginas anómalas con OCR y retorna el texto extraído."""
    # Usar un archivo temporal con nombre es más robusto entre sistemas operativos.
    with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as temp_pdf:
        temp_file_path = temp_pdf.name
        writer = PdfWriter()
        
        logging.info(f"Creando PDF temporal con {len(anomalous_pages_indices)} página(s) anómala(s) para OCR.")
        # Agregamos las páginas anómalas al PDF temporal
        for idx in anomalous_pages_indices:
            writer.add_page(reader.pages[idx])
        
        writer.write(temp_pdf)
    
    ocr_texts = []
    try:
        # Llamamos a Azure para procesar el PDF con todas las páginas anómalas
        result = analyze_document_read(client, temp_file_path)
        
        # Almacenamos el texto de cada página
        # Aseguramos que cada 'página' en el resultado tenga su texto concatenado.
        ocr_texts = [" ".join([line.content for line in page.lines]) for page in result.pages]
    except Exception as e:
        logging.error(f"Falló la llamada a Azure OCR en el archivo temporal: {e}")
        # En caso de error, devolvemos una lista de strings vacíos para no romper el flujo
        ocr_texts = ["" for _ in anomalous_pages_indices]
    finally:
        # Eliminamos el archivo temporal
        os.remove(temp_file_path)
        logging.info(f"Archivo temporal '{temp_file_path}' eliminado.")
    
    return ocr_texts

# --- FIN: Tus 4 funciones de Azure ---

# --- REESCRIBIMOS la función remove_lexnet_pages para integrar el OCR ---
def remove_lexnet_pages(input_pdf_path, output_pdf_path, azure_client=None):
    """
    Lee un PDF. Para cada página:
    1. Intenta extraer texto con pypdf.
    2. Si el texto es anómalo Y hay cliente de Azure, marca la página para OCR.
    3. Procesa en lote las páginas anómalas con Azure.
    4. Reconstruye el texto completo del documento.
    5. Elimina las páginas que contienen el marcador LexNET (basado en el texto final).
    6. Guarda el PDF resultante sin las páginas LexNET.
    """
    try:
        reader = PdfReader(input_pdf_path)
        writer = PdfWriter()
        
        all_pages_text = [] # Almacenará el texto final de cada página
        anomalous_indices = [] # Almacenará los índices de las páginas que necesitan OCR

        # 1. Primer pase: Análisis de texto y detección de anomalías
        logging.info(f"Analizando páginas de '{input_pdf_path.name}' con pypdf...")
        for i, page in enumerate(reader.pages):
            pypdf_text = page.extract_text()
            if azure_client and is_anomalous_text(pypdf_text):
                logging.info(f"  -> Página {i+1} marcada para OCR de Azure.")
                anomalous_indices.append(i)
                all_pages_text.append(None) # Marcador de posición para el texto OCR
            else:
                all_pages_text.append(pypdf_text or "") # Usar texto pypdf (o vacío si es None)

        # 2. Segundo pase (condicional): Procesar con Azure OCR si es necesario
        if anomalous_indices:
            logging.info(f"Procesando {len(anomalous_indices)} página(s) con Azure OCR...")
            ocr_texts = process_pages_with_ocr(anomalous_indices, reader, azure_client)
            
            # Rellenar los textos faltantes con los resultados del OCR
            for list_idx, original_page_idx in enumerate(anomalous_indices):
                if list_idx < len(ocr_texts):
                    all_pages_text[original_page_idx] = ocr_texts[list_idx]
                else:
                    logging.error(f"Discrepancia de resultados de OCR para página {original_page_idx}. Dejando como texto vacío.")
                    all_pages_text[original_page_idx] = ""

        # 3. Tercer pase: Ensamblaje final del PDF sin páginas LexNET
        logging.info("Ensamblando PDF final sin páginas de LexNET...")
        pages_removed_count = 0
        for i, page in enumerate(reader.pages):
            final_text = all_pages_text[i]
            # Comprobación del marcador LexNET en el texto final (de pypdf o Azure)
            if final_text and LEXNET_MARKER in final_text:
                logging.info(f"  -> Eliminando página {i+1} de '{input_pdf_path.name}' (contiene '{LEXNET_MARKER}')")
                pages_removed_count += 1
            else:
                writer.add_page(page)

        # Guardar el PDF resultante (lógica sin cambios)
        # ... (código para guardar el writer en output_pdf_path) ...
        # (Este bloque es idéntico al que ya tenías)
        if len(writer.pages) == 0:
            logging.warning(f"¡El PDF '{input_pdf_path.name}' quedó vacío! No se generará archivo.")
            output_pdf_path.unlink(missing_ok=True)
            return False
            
        output_pdf_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_pdf_path, "wb") as f_out:
            writer.write(f_out)
        
        logging.info(f"PDF temporal guardado en: {output_pdf_path} (se eliminaron {pages_removed_count} páginas de LexNET).")
        return True

    except Exception as e:
        logging.error(f"Error CRÍTICO en el pre-procesamiento de PDF '{input_pdf_path.name}': {e}")
        output_pdf_path.unlink(missing_ok=True)
        return False

# --- Modifica la llamada en `process_folder` ---
def process_folder(raw_folder_path, processed_folder, temp_pdf_folder, label, azure_client=None):
    # ... (inicio de la función sin cambios) ...
    for pdf_file in pdf_files:
        # ... (código para definir original_stem, temp_pdf_path) ...

        # Paso 1: Eliminar páginas con pypdf (y ahora potencialmente con OCR)
        # >>> PASAMOS EL CLIENTE DE AZURE AQUÍ <<<
        success_pypdf = remove_lexnet_pages(pdf_file, temp_pdf_path, azure_client)

        # ... (resto de la función `process_folder` sin cambios) ...


# --- Modifica la función `main` para manejar las credenciales y el cliente ---
def main():
    logging.info("--- Iniciando Script de Preprocesamiento (con filtro LexNET y OCR de Azure) ---")
    
    # --- NUEVO: Carga de credenciales y creación del cliente de Azure ---
    azure_client = None
    endpoint = os.getenv("AZURE_DI_ENDPOINT")
    key = os.getenv("AZURE_DI_KEY")

    if endpoint and key:
        logging.info("Credenciales de Azure encontradas en las variables de entorno.")
        try:
            azure_client = create_azure_client(endpoint, key)
        except Exception as e:
            logging.error(f"No se pudo crear el cliente de Azure. El OCR no estará disponible. Error: {e}")
    else:
        logging.warning("No se encontraron las variables de entorno AZURE_DI_ENDPOINT o AZURE_DI_KEY.")
        logging.warning("El script se ejecutará sin la capacidad de OCR para páginas escaneadas/cifradas.")

    # ... (resto de la función main: limpiar directorios, etc.) ...
    
    # ... (bucle for folder_name, label in LABEL_MAP.items():) ...
        # >>> PASAMOS EL CLIENTE DE AZURE AQUÍ <<<
        folder_data = process_folder(raw_folder_path, PROCESSED_FOLDER, TEMP_PDF_FOLDER, label, azure_client)
        # ... (resto del bucle y de la función main sin cambios) ...


if __name__ == "__main__":
    main()


Cómo Usarlo y Explicación de los Cambios
 * Manejo de Credenciales (¡IMPORTANTE!):
   * El script ahora busca tus credenciales de Azure en variables de entorno. Esto es una práctica de seguridad estándar para no escribir claves secretas en el código.
   * Antes de ejecutar el script, necesitas configurar estas variables en tu terminal:
     En Linux / macOS:
     export AZURE_DI_ENDPOINT="https://TUDOCUMENTINTELLIGENCE-ENDPOINT.cognitiveservices.azure.com/"
export AZURE_DI_KEY="TU_CLAVE_SECRETA_AQUI"

     En Windows (Command Prompt):
     set AZURE_DI_ENDPOINT="https://TUDOCUMENTINTELLIGENCE-ENDPOINT.cognitiveservices.azure.com/"
set AZURE_DI_KEY="TU_CLAVE_SECRETA_AQUI"

     En Windows (PowerShell):
     $env:AZURE_DI_ENDPOINT="https://TUDOCUMENTINTELLIGENCE-ENDPOINT.cognitiveservices.azure.com/"
$env:AZURE_DI_KEY="TU_CLAVE_SECRETA_AQUI"

   * Si el script no encuentra estas variables, simplemente mostrará una advertencia y continuará sin usar OCR. Esto lo hace flexible y no te obliga a tener credenciales para ejecutarlo.
 * Lógica de Integración en remove_lexnet_pages:
   * Paso 1 (Análisis): El script primero recorre el PDF página por página usando pypdf. Si el texto extraído parece "anómalo" (según tu función is_anomalous_text), guarda el número de esa página en una lista y deja su texto como "pendiente".
   * Paso 2 (OCR en Lote): Si se encontraron páginas anómalas, se llama a tu función process_pages_with_ocr. Esta crea un único PDF temporal con solo esas páginas y hace una sola llamada a Azure, lo cual es muy eficiente.
   * Paso 3 (Ensamblaje): Finalmente, el script tiene el "mejor" texto disponible para cada página (ya sea de pypdf o de Azure OCR). Con este texto final, realiza la comprobación de Mensaje LexNET - y reconstruye el PDF sin las páginas no deseadas.
 * Flujo de Datos:
   * main crea el cliente de Azure (si puede) y lo pasa a process_folder.
   * process_folder lo recibe y lo pasa a remove_lexnet_pages.
   * remove_lexnet_pages lo usa cuando lo necesita.
Esta integración es robusta, eficiente y desacoplada. El OCR solo se usa cuando es necesario, no interrumpe el flujo si no hay credenciales y respeta tu lógica de agrupar llamadas a la API.


2****†****


¡Excelente observación! Tienes toda la razón, es un fallo crítico en el flujo que propuse. Pido disculpas por no haberlo detectado.
El problema, como bien señalas, es:
 * Usamos pypdf y Azure OCR para obtener el texto de alta calidad de las páginas escaneadas.
 * Pero luego, para generar el Markdown, le pasamos a markitdown el PDF temporal. Este PDF, aunque ya no tiene las páginas de LexNET, sigue conteniendo las páginas escaneadas como imágenes, de las cuales markitdown probablemente tampoco podrá extraer texto correctamente.
 * Resultado: El texto del OCR de Azure se usa para el chequeo de "LexNET", pero se descarta y nunca llega al archivo .md final.
Necesitamos una nueva estrategia que garantice que el texto del OCR sea el que se utilice para generar el archivo .md en los casos necesarios, mientras seguimos aprovechando markitdown para los documentos "normales".
La Mejor Manera de Integrarlo: Un Flujo Híbrido
La solución más robusta es tener dos caminos dentro de process_folder: uno para los PDFs "normales" (que usan markitdown) y otro para los PDFs que requieren OCR (que escriben el texto de Azure directamente).
Aquí está el código modificado para src/preprocessing.py que implementa esta lógica.
# --- (Importaciones y constantes sin cambios, asegúrate de tener todo lo anterior) ---
# ...
import os
import subprocess
import pandas as pd
from pathlib import Path
import logging
import re
from pypdf import PdfReader, PdfWriter
import shutil
import tempfile
from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import DocumentIntelligenceClient
# ...

# --- (Tus 4 funciones de Azure + clean_text aquí, sin cambios) ---
# create_azure_client, analyze_document_read, is_anomalous_text, process_pages_with_ocr, clean_text
# ...

# --- NUEVA Función de Lógica Central ---
def get_definitive_text(pdf_path, azure_client):
    """
    Obtiene el texto definitivo de un PDF, usando Azure OCR como fallback.
    También filtra internamente las páginas que contienen el marcador LexNET.
    Retorna una única cadena con el texto completo del documento.
    """
    if not azure_client:
        logging.warning("Se intentó obtener texto definitivo con OCR, pero el cliente de Azure no está disponible.")
        # Fallback: intentar extraer texto solo con pypdf y devolverlo
        try:
            reader = PdfReader(pdf_path)
            full_text = ""
            for page in reader.pages:
                text = page.extract_text() or ""
                if LEXNET_MARKER not in text:
                    full_text += text + "\n\n" # Añadir separador de página
            return full_text
        except Exception as e:
            logging.error(f"Error leyendo PDF con pypdf en modo fallback: {e}")
            return ""

    # Flujo completo con OCR
    try:
        reader = PdfReader(pdf_path)
        all_pages_text = []
        anomalous_indices = []

        # 1. Análisis con pypdf
        for i, page in enumerate(reader.pages):
            pypdf_text = page.extract_text()
            if is_anomalous_text(pypdf_text):
                anomalous_indices.append(i)
                all_pages_text.append(None)
            else:
                all_pages_text.append(pypdf_text or "")

        # 2. OCR en lote si es necesario
        if anomalous_indices:
            logging.info(f"'{pdf_path.name}' requiere OCR. Obteniendo texto desde Azure...")
            ocr_texts = process_pages_with_ocr(anomalous_indices, reader, azure_client)
            for list_idx, original_page_idx in enumerate(anomalous_indices):
                if list_idx < len(ocr_texts):
                    all_pages_text[original_page_idx] = ocr_texts[list_idx]
                else:
                    all_pages_text[original_page_idx] = ""

        # 3. Ensamblaje del texto final, FILTRANDO páginas LexNET
        final_content_parts = []
        for text in all_pages_text:
            if text and LEXNET_MARKER not in text:
                final_content_parts.append(text)
        
        return "\n\n".join(final_content_parts) # Unir páginas con un doble salto de línea

    except Exception as e:
        logging.error(f"Error en get_definitive_text para '{pdf_path.name}': {e}")
        return "" # Devolver vacío en caso de error grave

# --- Función para crear PDF sin páginas LexNET (para el flujo Markitdown) ---
def create_pdf_without_lexnet(input_pdf_path, output_pdf_path):
    """
    Crea una copia de un PDF eliminando solo las páginas con el marcador LexNET.
    Usa únicamente pypdf para la detección de texto. No usa OCR.
    """
    try:
        reader = PdfReader(input_pdf_path)
        writer = PdfWriter()
        for page in reader.pages:
            text = page.extract_text() or ""
            if LEXNET_MARKER not in text:
                writer.add_page(page)
        
        if len(writer.pages) > 0:
            with open(output_pdf_path, "wb") as f_out:
                writer.write(f_out)
            return True
        else:
            logging.warning(f"PDF '{input_pdf_path.name}' quedó vacío tras quitar páginas LexNET.")
            return False
    except Exception as e:
        logging.error(f"Error en create_pdf_without_lexnet para '{input_pdf_path.name}': {e}")
        return False


# --- REESCRIBIMOS `process_folder` para implementar el flujo HÍBRIDO ---
def process_folder(raw_folder_path, processed_folder, temp_pdf_folder, label, azure_client=None):
    # ... (inicio de la función sin cambios) ...

    for pdf_file in pdf_files:
        logging.info(f"Procesando archivo: {pdf_file.name}")
        original_stem = pdf_file.stem
        md_final_path = processed_folder / f"{original_stem}.md"

        # --- Decisión: ¿Este PDF necesita OCR o puede usar Markitdown? ---
        # Verificamos si alguna página es anómala ANTES de decidir el camino.
        needs_ocr = False
        if azure_client:
            try:
                reader = PdfReader(pdf_file)
                for page in reader.pages:
                    if is_anomalous_text(page.extract_text()):
                        needs_ocr = True
                        break # Encontramos una, es suficiente para decidir
            except Exception as e:
                logging.error(f"No se pudo pre-analizar '{pdf_file.name}' para decidir ruta. Se usará Markitdown por defecto. Error: {e}")

        final_text = None
        # --- RUTA 1: Se necesita OCR ---
        if needs_ocr:
            logging.info(f"'{pdf_file.name}' es anómalo. Usando ruta de extracción directa con OCR de Azure.")
            # Obtenemos el texto definitivo, que ya viene filtrado de páginas LexNET
            final_text = get_definitive_text(pdf_file, azure_client)
            # No se necesita markitdown ni PDF temporal. El texto es el producto final.

        # --- RUTA 2: PDF normal, usar Markitdown ---
        else:
            logging.info(f"'{pdf_file.name}' parece normal. Usando ruta de Markitdown.")
            temp_pdf_path = temp_pdf_folder / f"{original_stem}_temp.pdf"
            
            # 1. Crear PDF temporal sin páginas LexNET
            if create_pdf_without_lexnet(pdf_file, temp_pdf_path):
                # 2. Convertir el PDF temporal a MD
                md_path_from_converter = convert_pdf_to_md(temp_pdf_path, processed_folder, original_stem)
                # 3. Limpiar el temporal
                temp_pdf_path.unlink(missing_ok=True)
                
                # 4. Leer el texto del MD generado para tenerlo en la variable 'final_text'
                if md_path_from_converter:
                    try:
                        with open(md_path_from_converter, 'r', encoding='utf-8') as f:
                           final_text = f.read()
                    except Exception as e:
                        logging.error(f"No se pudo leer el MD generado por Markitdown para '{pdf_file.name}': {e}")

        # --- Paso final común: Limpiar y guardar el texto en el archivo .md ---
        if final_text is not None:
            # Aplicar limpieza final y guardar
            cleaned_text = clean_text(final_text)
            try:
                with open(md_final_path, 'w', encoding='utf-8') as f:
                    f.write(cleaned_text)
                logging.info(f"Archivo final '{md_final_path.name}' guardado/sobrescrito.")
                
                # Añadir la ruta a la lista de datos para el CSV
                data.append({
                    'source_pdf': pdf_file.name,
                    'markdown_file': md_final_path.name,
                    'markdown_path': str(md_final_path),
                    'label': label
                })
            except Exception as e:
                logging.error(f"Error al guardar el MD final '{md_final_path.name}': {e}")
        else:
            logging.warning(f"No se pudo generar contenido final para '{pdf_file.name}'. No se creará archivo .md ni se añadirá al dataset.")
    
    return data

# --- (Las funciones `main`, `convert_pdf_to_md` y las de Azure no necesitan más cambios) ---
# ...

Explicación de la Nueva Lógica
 * Función get_definitive_text:
   * Esta es la nueva función central para los casos de OCR.
   * Recibe un PDF y el cliente de Azure.
   * Implementa la lógica que diseñamos antes (análisis con pypdf, lote de OCR con Azure, fusión de textos).
   * Crucialmente, también filtra las páginas con "Mensaje LexNET -" basándose en el texto final y de alta calidad.
   * Su única salida es una cadena de texto gigante con todo el contenido válido del documento, listo para ser guardado.
 * Función create_pdf_without_lexnet:
   * Esta es una versión simplificada de la antigua remove_lexnet_pages. Su única responsabilidad es crear una copia de un PDF sin las páginas de LexNET, usando solo pypdf. Es para el flujo que no usa OCR.
 * El Orquestador: process_folder con Flujo Híbrido:
   * Para cada PDF, primero hace una revisión rápida para ver si alguna de sus páginas parece anómala.
   * Si detecta una anomalía (y hay cliente de Azure):
     * Entra en la "Ruta OCR".
     * Llama a get_definitive_text para obtener el texto de alta calidad directamente.
     * No usa markitdown ni PDFs temporales. El texto de Azure es el producto final.
   * Si el PDF parece normal:
     * Entra en la "Ruta Markitdown".
     * Usa create_pdf_without_lexnet para generar el PDF temporal limpio.
     * Llama a markitdown para que haga su trabajo de extracción (que puede preservar mejor la maquetación).
     * Lee el texto del .md resultante.
   * Paso Final Común: Al final, ambas rutas convergen. Tenemos una variable final_text que contiene el mejor texto posible (ya sea de OCR o de markitdown). Este texto se limpia con clean_text y se guarda en el archivo .md final.
Este nuevo enfoque soluciona el problema de raíz, asegurando que el texto de alta calidad del OCR es el que realmente se guarda y se utiliza en los documentos que lo necesitan, mientras se mantiene el uso de markitdown para los casos estándar.

